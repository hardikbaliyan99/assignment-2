{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "QD7Mm8wKxTgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It is particularly effective for high-dimensional spaces and is widely used in machine learning.\n",
        "\n",
        "Types of SVM:\n",
        "\n",
        "Linear SVM:\n",
        "Used when data is linearly separable (can be divided by a straight line).\n",
        "\n",
        "Non-Linear SVM:\n",
        "\n",
        "Used when data is not linearly separable.\n",
        "Uses kernel tricks (like polynomial or radial basis function (RBF) kernels) to map data into higher dimensions where a linear separation is possible.\n"
      ],
      "metadata": {
        "id": "vTyfCUsPxXef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2RFSMj5vxROm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is the difference between Hard Margin and Soft Margin SVM\n"
      ],
      "metadata": {
        "id": "5wyYRvpPxmKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference between Hard Margin SVM and Soft Margin SVM lies in how strictly the model enforces class separation and how it handles overlapping data points."
      ],
      "metadata": {
        "id": "2wkP0Pvkyq3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Hard Margin SVM\n",
        "Definition:\n",
        "Hard Margin SVM is used when the data is perfectly linearly separable (i.e., there exists a hyperplane that completely separates the classes with no misclassification).\n",
        "It tries to maximize the margin between the classes without allowing any misclassified points.\n",
        "Mathematical Formulation:\n",
        "min\n",
        "⁡\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "min\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "Subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "where:\n",
        "\n",
        "𝑤\n",
        "w is the weight vector,\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  are the data points,\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  are the class labels (\n",
        "+\n",
        "1\n",
        "+1 or\n",
        "−\n",
        "1\n",
        "−1),\n",
        "𝑏\n",
        "b is the bias term.\n",
        "Limitations:\n",
        "Sensitive to noise and outliers: If there is even one misclassified or noisy data point, Hard Margin SVM fails.\n",
        "Only works when data is perfectly separable.\n",
        "2. Soft Margin SVM\n",
        "Definition:\n",
        "Soft Margin SVM allows some misclassification by introducing a slack variable (\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        " ) for each data point.\n",
        "It balances maximizing the margin and minimizing misclassification.\n",
        "Mathematical Formulation:\n",
        "min\n",
        "⁡\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝑖\n",
        "𝜉\n",
        "𝑖\n",
        "min\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C\n",
        "i\n",
        "∑\n",
        "​\n",
        " ξ\n",
        "i\n",
        "​\n",
        "\n",
        "Subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "−\n",
        "𝜉\n",
        "𝑖\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "where:\n",
        "\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        "  is the slack variable (how much a point violates the margin),\n",
        "𝐶\n",
        "C is a regularization parameter controlling the trade-off between maximizing margin and minimizing errors.\n"
      ],
      "metadata": {
        "id": "l7_XkHzGyuQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What is the mathematical intuition behind SVM"
      ],
      "metadata": {
        "id": "NopYD_9zy0aF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SVM aims to find the optimal hyperplane that best separates different classes in a dataset while maximizing the margin between the closest data points (support vectors). Let's break it down mathematically.\n",
        "\n",
        "\n",
        ". Defining the Hyperplane\n",
        "\n",
        "A hyperplane is a decision boundary that separates different classes. In an\n",
        "𝑛\n",
        "n-dimensional space, the equation of a hyperplane is:\n",
        "\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "0\n",
        "w⋅x+b=0\n",
        "where:\n",
        "\n",
        "𝑤\n",
        "w is the weight vector (normal to the hyperplane),\n",
        "𝑥\n",
        "x is the input feature vector,\n",
        "𝑏\n",
        "b is the bias term.\n",
        "For classification:\n",
        "\n",
        "If\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        ">\n",
        "0\n",
        "w⋅x+b>0, classify as +1.\n",
        "If\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "<\n",
        "0\n",
        "w⋅x+b<0, classify as -1.\n",
        "2. Margin and Optimal Hyperplane\n",
        "\n",
        "What is the margin?\n",
        "\n",
        "The margin is the dis\n",
        "tance between the hyperplane and the closest points from both classes (called support vectors). The goal of SVM is to maximize this margin.\n",
        "\n",
        "Mathematical Formulation of the Margin:\n",
        "For a correctly classified point\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " , the decision rule is:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "where\n",
        "𝑦\n",
        "𝑖\n",
        "∈\n",
        "{\n",
        "+\n",
        "1\n",
        ",\n",
        "−\n",
        "1\n",
        "}\n",
        "y\n",
        "i\n",
        "​\n",
        " ∈{+1,−1} represents class labels.\n",
        "\n",
        "The distance from any point\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  to the hyperplane is:\n",
        "\n",
        "∣\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        "∣\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "∣∣w∣∣\n",
        "∣w⋅x\n",
        "i\n",
        "​\n",
        " +b∣\n",
        "​\n",
        "\n",
        "The margin (distance between the two closest points from different classes) is:\n",
        "\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "∣∣w∣∣\n",
        "2\n",
        "​\n",
        "\n",
        "Objective: Maximizing the Margin\n",
        "Since maximizing the margin is equivalent to minimizing\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "∣∣w∣∣, the optimization problem becomes:\n",
        "\n",
        "min\n",
        "⁡\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "min\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "subject to the constraint:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "3. Handling Non-Linearly Separable Data (Soft Margin SVM)\n",
        "In real-world scenarios, perfect separation is rare. We introduce slack variables (\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        " ) to allow misclassification:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "−\n",
        "𝜉\n",
        "𝑖\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "and modify our objective function:\n",
        "\n",
        "min\n",
        "⁡\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝜉\n",
        "𝑖\n",
        "min\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C∑ξ\n",
        "i\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝐶\n",
        "C is a regularization parameter that controls the trade-off between maximizing the margin and allowing misclassifications.\n",
        "Larger\n",
        "𝐶\n",
        "C → Less misclassification, but smaller margin.\n",
        "Smaller\n",
        "𝐶\n",
        "C → More misclassification, but larger margin.\n",
        "4. Dual Form and the Kernel Trick\n",
        "When data is not linearly separable, we transform it into a higher-dimensional space where it becomes linearly separable. Instead of working directly in this high-dimensional space, we use the Kernel Trick.\n",
        "\n",
        "The SVM optimization problem in dual form is:\n",
        "\n",
        "max\n",
        "⁡\n",
        "∑\n",
        "𝛼\n",
        "𝑖\n",
        "−\n",
        "1\n",
        "2\n",
        "∑\n",
        "∑\n",
        "𝛼\n",
        "𝑖\n",
        "𝛼\n",
        "𝑗\n",
        "𝑦\n",
        "𝑖\n",
        "𝑦\n",
        "𝑗\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "max∑α\n",
        "i\n",
        "​\n",
        " −\n",
        "2\n",
        "1\n",
        "​\n",
        " ∑∑α\n",
        "i\n",
        "​\n",
        " α\n",
        "j\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " y\n",
        "j\n",
        "​\n",
        " K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )\n",
        "subject to:\n",
        "\n",
        "∑\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "=\n",
        "0\n",
        ",\n",
        "0\n",
        "≤\n",
        "𝛼\n",
        "𝑖\n",
        "≤\n",
        "𝐶\n",
        "∑α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " =0,0≤α\n",
        "i\n",
        "​\n",
        " ≤C\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        "  are Lagrange multipliers.\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " ) is a kernel function that computes the inner product in the transformed space.\n",
        "Common kernels:\n",
        "\n",
        "Linear Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        "  (for linearly separable data)\n",
        "Polynomial Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=(x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        " +c)\n",
        "d\n",
        "\n",
        "Radial Basis Function (RBF) Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "𝑗\n",
        "∣\n",
        "∣\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=exp(−γ∣∣x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∣∣\n",
        "2\n",
        " ) (for complex decision boundaries)\n",
        "5. Final Decision Function\n",
        "Once we solve for\n",
        "𝑤\n",
        "w and\n",
        "𝑏\n",
        "b, the classification rule for a new input\n",
        "𝑥\n",
        "x is:\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "sign\n",
        "(\n",
        "∑\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        ")\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "f(x)=sign(∑α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " K(x\n",
        "i\n",
        "​\n",
        " ,x)+b)\n"
      ],
      "metadata": {
        "id": "QcYbiTnVzbBX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpUabJxFw-d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.What is the role of Lagrange Multipliers in SVM\n",
        "\n",
        "Lagrange multipliers are used in Support Vector Machines (SVM) to transform the constrained optimization problem into a more manageable form using Lagrangian duality. This helps in efficiently finding the optimal hyperplane that separates the data.\n",
        "\n",
        "1. Why Do We Need Lagrange Multipliers in SVM?\n",
        "SVM aims to maximize the margin between classes while satisfying the constraint that all data points are classified correctly. The optimization problem is:\n",
        "\n",
        "Primal Formulation (Constrained Optimization)\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "w,b\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "where:\n",
        "\n",
        "𝑤\n",
        "w is the weight vector,\n",
        "𝑏\n",
        "b is the bias,\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  are the training data points,\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  are the class labels (\n",
        "+\n",
        "1\n",
        "+1 or\n",
        "−\n",
        "1\n",
        "−1).\n",
        "This is a constrained optimization problem, which is difficult to solve directly. To handle this, we introduce Lagrange multipliers.\n",
        "\n",
        "2. Using Lagrange Multipliers\n",
        "We introduce Lagrange multipliers\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        "  (one for each constraint) and construct the Lagrangian function:\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝛼\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "𝛼\n",
        "𝑖\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "−\n",
        "1\n",
        "]\n",
        "L(w,b,α)=\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " −\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)−1]\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "α\n",
        "i\n",
        "​\n",
        " ≥0 are the Lagrange multipliers.\n",
        "The term inside the summation enforces the constraint\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1.\n",
        "3. Solving the Optimization Problem (Dual Form)\n",
        "Step 1: Compute Partial Derivatives\n",
        "To find the optimal solution, we take derivatives of\n",
        "𝐿\n",
        "(\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝛼\n",
        ")\n",
        "L(w,b,α) with respect to\n",
        "𝑤\n",
        "w and\n",
        "𝑏\n",
        "b and set them to zero:\n",
        "\n",
        "Derivative w.r.t.\n",
        "𝑤\n",
        "w:\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑤\n",
        "=\n",
        "𝑤\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "∂w\n",
        "∂L\n",
        "​\n",
        " =w−\n",
        "i\n",
        "∑\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " x\n",
        "i\n",
        "​\n",
        " =0\n",
        "⇒\n",
        "𝑤\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "⇒w=\n",
        "i\n",
        "∑\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " x\n",
        "i\n",
        "​\n",
        "\n",
        "(This shows that\n",
        "𝑤\n",
        "w is a linear combination of training points weighted by\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        " ).\n",
        "\n",
        "Derivative w.r.t.\n",
        "𝑏\n",
        "b:\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑏\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "∂b\n",
        "∂L\n",
        "​\n",
        " =\n",
        "i\n",
        "∑\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " =0\n",
        "(This ensures the sum of weighted class labels is zero)."
      ],
      "metadata": {
        "id": "pw9DCRoe0B1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.What are Support Vectors in SVM\n",
        "Support Vectors in SVM\n",
        "Support vectors are the data points closest to the decision boundary (hyperplane) in a Support Vector Machine (SVM). These points are crucial because they define the margin of the classifier and directly influence the position of the decision boundary.\n",
        "\n",
        " Why Are Support Vectors Important?\n",
        "They determine the optimal hyperplane: The decision boundary is uniquely defined by these points.\n",
        "They affect generalization: Since the margin is maximized based on these points, they help improve the model's ability to classify new data correctly.\n",
        "Only support vectors matter: Other training points do not contribute directly to defining the hyperplane."
      ],
      "metadata": {
        "id": "n_U8W1Tq0flU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.What is a Support Vector Classifier (SVC)"
      ],
      "metadata": {
        "id": "QgVl7WN-02pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Classifier (SVC) – The Classification Version of SVM\n",
        "A Support Vector Classifier (SVC) is the classification variant of Support Vector Machines (SVM). It finds an optimal hyperplane that best separates different classes while maximizing the margin between them"
      ],
      "metadata": {
        "id": "PsdbLnKi1CxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.What is a Support Vector Regressor (SVR)\n",
        "Support Vector Regressor (SVR) – The Regression Version of SVM\n",
        "A Support Vector Regressor (SVR) is the regression counterpart of Support Vector Machines (SVM). Instead of classifying data into categories like Support Vector Classifier (SVC), SVR predicts continuous values while maintaining the margin-based approach of SVM.\n",
        "\n"
      ],
      "metadata": {
        "id": "10OnDcJX1Gqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 8.What is the Kernel Trick in SVM"
      ],
      "metadata": {
        "id": "89--HACp1apz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kernel Trick is a fundamental technique used in Support Vector Machines (SVMs) to handle non-linearly separable data by transforming it into a higher-dimensional space, where it becomes linearly separable. Instead of explicitly computing the transformation, the Kernel Trick efficiently calculates the inner product in the transformed space, saving computational cost.\n",
        "\n",
        "1. Why Do We Need the Kernel Trick?\n",
        "SVMs work well with linearly separable data, where a straight line (2D) or a hyperplane (3D+) can separate the classes. However, many real-world datasets are not linearly separable.\n",
        "\n",
        "For example:\n",
        "\n",
        "In 2D, classes may form concentric circles.\n",
        "In 3D, classes may follow a complex curved boundary.\n",
        "To handle this, we map the data to a higher-dimensional space where it can be linearly separated.\n",
        "\n"
      ],
      "metadata": {
        "id": "BdO9Dhof1r1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.: Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        " Linear Kernel\n",
        "Best for: Data that is linearly separable.\n",
        "Equation:\n",
        "\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        "\n",
        "Computational Complexity: Low (Fastest among all kernels).\n",
        "Hyperparameters: Only C (regularization parameter).\n",
        "Example Use Case:\n",
        "Text classification (e.g., spam detection, sentiment analysis).\n",
        "High-dimensional datasets (where linear separation is more likely).\n",
        "Polynomial Kernel\n",
        "Best for: Data that has a polynomial relationship between features.\n",
        "Equation:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=(x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        " +c)\n",
        "d\n",
        "\n",
        "Computational Complexity: Medium to High (Higher-degree polynomials increase complexity).\n",
        "Hyperparameters:\n",
        "Degree (d): Controls flexibility (higher degrees fit more complex data).\n",
        "Constant (c): Adjusts impact of higher-order terms.\n",
        "Regularization (C): Controls margin-width tradeoff."
      ],
      "metadata": {
        "id": "UVOg6rK31vUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.What is the effect of the C parameter in SVM\n",
        "Effect of the\n",
        "𝐶\n",
        "C Parameter in SVM\n",
        "The\n",
        "𝐶\n",
        "C parameter in Support Vector Machines (SVMs) is a regularization hyperparameter that controls the trade-off between:\n",
        "\n",
        "Maximizing the margin (simpler, more generalized model).\n",
        "Minimizing classification errors (fitting the training data more tightly).\n"
      ],
      "metadata": {
        "id": "JkzkgLbo2KAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "Role of the Gamma (\n",
        "𝛾\n",
        "γ) Parameter in RBF Kernel SVM\n",
        "The Gamma (\n",
        "𝛾\n",
        "γ) parameter in Radial Basis Function (RBF) Kernel SVM controls how far the influence of a single training example reaches. It determines how much each data point contributes to the decision boundary."
      ],
      "metadata": {
        "id": "C8QUKhP32Yzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. What is the Naïve Bayes classifier, and why is it called \"Naïve"
      ],
      "metadata": {
        "id": "Tn3LHgmp2oEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes is a probabilistic machine learning algorithm based on Bayes' Theorem. It is used for classification tasks and is particularly effective in text classification, spam detection, and sentiment analysis.\n",
        "\n",
        "It assumes that features (input variables) are independent, which simplifies calculations and makes it computationally efficient.\n",
        "\n",
        "Bayes’ Theorem Formula:\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(Y∣X)=\n",
        "P(X)\n",
        "P(X∣Y)⋅P(Y)\n",
        "​\n"
      ],
      "metadata": {
        "id": "5mbs66Mb2wCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. What is Bayes’ Theorem"
      ],
      "metadata": {
        "id": "2ScSceff2zk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' Theorem is a mathematical formula used to calculate conditional probability—the probability of an event occurring given that another event has already occurred. It is the foundation of Bayesian statistics and is widely used in machine learning, medical diagnosis, spam filtering, and fraud detection.\n",
        "\n",
        "1. Bayes’ Theorem Formula\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        "∣\n",
        "𝐵\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        "∣\n",
        "𝐴\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        ")\n",
        "P(A∣B)=\n",
        "P(B)\n",
        "P(B∣A)⋅P(A)\n",
        "​\n",
        "\n"
      ],
      "metadata": {
        "id": "mJQYhzYb23sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14.Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:"
      ],
      "metadata": {
        "id": "jLCXS-8d3Fd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differences Between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "Naïve Bayes classifiers are a family of probabilistic models based on Bayes’ Theorem, with different variations tailored to different types of data. The three main types are:\n",
        "\n",
        "Gaussian Naïve Bayes (GNB) → For continuous (numerical) data\n",
        "Multinomial Naïve Bayes (MNB) → For count-based text data\n",
        "Bernoulli Naïve Bayes (BNB) → For binary (0/1) feature data\n"
      ],
      "metadata": {
        "id": "go-dX0gU3N5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 15.When should you use Gaussian Naïve Bayes over other variants"
      ],
      "metadata": {
        "id": "KmkDdV_P3ZO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use Gaussian Naïve Bayes Over Other Variants\n",
        "Use Gaussian Naïve Bayes (GNB) when:\n",
        "\n",
        "Your dataset has continuous numerical features (e.g., height, weight, temperature, age).\n",
        "Your features follow (or approximately follow) a normal (Gaussian) distribution.\n",
        "You need a fast, scalable, and interpretable classifier.\n",
        "Your dataset is small and you need a model that performs well with limited training data.\n",
        "You are working with real-valued sensor data, such as medical diagnosis or weather forecasting.\n"
      ],
      "metadata": {
        "id": "zSY6IuZS3ya1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. What are the key assumptions made by Naïve Bayes4"
      ],
      "metadata": {
        "id": "tOMML1bm35qr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional Independence Assumption (The \"Naïve\" Assumption)\n",
        "🚀 Each feature is assumed to be independent given the class label.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "1\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "2\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        ".\n",
        ".\n",
        ".\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "𝑛\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "P(X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,...,X\n",
        "n\n",
        "​\n",
        " ∣Y)=P(X\n",
        "1\n",
        "​\n",
        " ∣Y)P(X\n",
        "2\n",
        "​\n",
        " ∣Y)...P(X\n",
        "n\n",
        "​\n",
        " ∣Y)\n",
        " Feature Importance Assumption\n",
        "\n",
        " Class Conditional Independence Assumption\n",
        "\n",
        "  Correct Probability Distribution Assumption\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Bpqin9SM37dY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17.What are the advantages and disadvantages of Naïve Bayes"
      ],
      "metadata": {
        "id": "Cw7m5CNL4Y_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of Naïve Bayes\n",
        "1️⃣ Fast and Efficient\n",
        "\n",
        "2️⃣ Works Well with Small Datasets\n",
        "\n",
        "3️⃣ Handles High-Dimensional Data Well\n",
        "\n",
        "4️⃣ Can Handle Missing Data\n",
        "\n",
        "5️⃣ Interpretable and Simple\n",
        "\n",
        "Disadvantages of Naïve Bayes:\n",
        "\n",
        "2️⃣ Zero Probability Problem\n",
        "\n",
        "3️⃣ Struggles with Continuous Data (for Multinomial/Bernoulli NB)\n",
        "\n",
        "4️⃣ Not Ideal for Complex Decision Boundaries\n",
        "\n",
        "5️⃣ Sensitive to Feature Irrelevance\n",
        "\n"
      ],
      "metadata": {
        "id": "gqr4dkhF4jyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18.: Why is Naïve Bayes a good choice for text classification\n",
        "\n",
        "1️⃣ Works Well with High-Dimensional Data (Large Vocabulary)\n",
        "\n",
        "2️⃣ Computationally Efficient and Fast\n",
        "\n",
        "3️⃣ Handles Small Datasets Well\n",
        "\n",
        "4️⃣ Handles Missing Words Well\n",
        "\n",
        "5️⃣ Naturally Handles Stop Words and Less Informative Words\n",
        "\n",
        "5️⃣ Naturally Handles Stop Words and Less Informative Words\n",
        "\n",
        "7️⃣ Provides Probabilistic Outputs"
      ],
      "metadata": {
        "id": "mcWQV8Sb5ADz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. Compare SVM and Naïve Bayes for classification tasks"
      ],
      "metadata": {
        "id": "_kFpIJaR5eHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Differences Between SVM and Naïve Bayes\n",
        "Criterion\tSVM (Support Vector Machine) 🏆\tNaïve Bayes (NB) ⚡\n",
        "Model Type\tDiscriminative\tGenerative\n",
        "Assumption\tFinds an optimal decision boundary\tAssumes independence between features\n",
        "Computation Speed\tSlower (especially on large datasets)\tFaster (suitable for real-time applications)\n",
        "Performance on Small Data\tWorks well, but may require tuning\tExcellent, works well even with very little data\n",
        "Performance on Large Data\tComputationally expensive, but effective\tScales well but may be less accurate\n",
        "High-Dimensional Data\tPerforms well but can be slow\tHandles high dimensions efficiently\n",
        "Text Classification\tWorks well, but slower\tOften outperforms SVM (faster and accurate)\n",
        "Non-Linearly Separable Data\tCan handle non-linearity with kernels\tStruggles if feature independence assumption is violated\n",
        "Handling Missing Data\tRequires preprocessing\tNaturally handles missing data well\n",
        "Interpretability\tHarder to interpret\tSimple and interpretable\n",
        "Overfitting Risk\tLow with proper regularization (C parameter)\tLow (especially with Laplace smoothing)\n",
        "Memory Usage\tHigh (stores support vectors)\tLow (stores probability distributions)\n"
      ],
      "metadata": {
        "id": "4RY1i3Ce5nTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.: How does Laplace Smoothing help in Naïve Bayes?\n",
        "How Does Laplace Smoothing Help in Naïve Bayes?\n",
        "🚨 Problem: The Zero Probability Issue\n",
        "Naïve Bayes estimates the probability of a feature\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  given a class\n",
        "𝑌\n",
        "Y as:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "=\n",
        "count\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        ",\n",
        "𝑌\n",
        ")\n",
        "∑\n",
        "count\n",
        "(\n",
        "𝑋\n",
        ",\n",
        "𝑌\n",
        ")\n",
        "P(X\n",
        "i\n",
        "​\n",
        " ∣Y)=\n",
        "∑count(X,Y)\n",
        "count(X\n",
        "i\n",
        "​\n",
        " ,Y)\n",
        "​\n",
        "\n",
        "However, if a word (feature) never appears in the training data for a class, its probability becomes zero. Since Naïve Bayes multiplies probabilities, this zero probability cancels out the entire prediction, making classification fail."
      ],
      "metadata": {
        "id": "19gR-_bz5rNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#                               Practical"
      ],
      "metadata": {
        "id": "jJQabQPG6CW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy"
      ],
      "metadata": {
        "id": "hNok07XB6Pt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Step 2: Split into training (80%) and testing (20%) data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM classifier\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')  # RBF Kernel SVM\n",
        "svm_model.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDwtzBg-6WhQ",
        "outputId": "32213b3f-93f8-49a3-dafc-f410bddea471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22.Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "#compare their accuracies:"
      ],
      "metadata": {
        "id": "ldnD1-8G6bD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target  # Features and labels\n",
        "\n",
        "# Step 2: Split into training (80%) and testing (20%) data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM classifiers\n",
        "# Linear Kernel SVM\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# RBF Kernel SVM\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate and compare accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"SVM (Linear Kernel) Accuracy: {accuracy_linear:.2f}\")\n",
        "print(f\"SVM (RBF Kernel) Accuracy: {accuracy_rbf:.2f}\")\n",
        "\n",
        "# Compare the models\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"Linear Kernel SVM performs better!\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"RBF Kernel SVM performs better!\")\n",
        "else:\n",
        "    print(\"Both kernels perform equally well.\")\n"
      ],
      "metadata": {
        "id": "zeG8K7sI6ioA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23.Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "#Squared Error (MSE):"
      ],
      "metadata": {
        "id": "pBFpjLPb6m3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target  # Features and target prices\n",
        "\n",
        "# Step 2: Split into training (80%) and testing (20%) data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVR model (RBF Kernel)\n",
        "svr_model = SVR(kernel='rbf', C=100, gamma='scale')  # C=100 for better fitting\n",
        "svr_model.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)  # Root Mean Squared Error (optional)\n",
        "\n",
        "print(f\"SVR Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"SVR Root Mean Squared Error: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "6nC-CORz6xFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. : Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "#boundary:"
      ],
      "metadata": {
        "id": "BgIWUcNy6zuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.svm import SVC\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "# Step 1: Generate synthetic data (moon-shaped, non-linearly separable)\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Train SVM with a Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Step 3: Visualize the decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_decision_regions(X, y, clf=svm_poly, legend=2)\n",
        "plt.title(\"SVM with Polynomial Kernel (Degree = 3)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WLPey4vu67Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#.25 Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy:**bold text**"
      ],
      "metadata": {
        "id": "pFe6bQo96916"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # Features and labels\n",
        "\n",
        "# Step 2: Split into training (80%) and testing (20%) data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Gaussian Naïve Bayes Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "GuyiHw9s7F7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "#Newsgroups dataset."
      ],
      "metadata": {
        "id": "DTWxki1H7JRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the 20 Newsgroups dataset (subset for faster training)\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']  # Select 4 categories\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Step 2: Convert text data into numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Limit vocabulary size\n",
        "X = vectorizer.fit_transform(newsgroups.data)  # Transform text into TF-IDF features\n",
        "y = newsgroups.target  # Labels\n",
        "\n",
        "# Step 3: Split data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the Multinomial Naïve Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Multinomial Naïve Bayes Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "nNvaNUiF7eNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 27. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "#boundaries visually"
      ],
      "metadata": {
        "id": "M7ZonhC67iZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.svm import SVC\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "# Step 1: Generate synthetic data (moon-shaped, non-linearly separable)\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Train SVM classifiers with different C values\n",
        "C_values = [0.1, 1, 10]\n",
        "svm_models = [SVC(kernel='rbf', C=C, gamma='scale').fit(X, y) for C in C_values]\n",
        "\n",
        "# Step 3: Visualize decision boundaries\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "for i, (C, model) in enumerate(zip(C_values, svm_models), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    plot_decision_regions(X, y, clf=model, legend=2)\n",
        "    plt.title(f\"SVM Decision Boundary (C={C})\")\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y_f-h_ZV7qYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   27.Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "#boundaries visually="
      ],
      "metadata": {
        "id": "HHdEP_z4Rk-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different values of C to compare\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, ax):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "    ax.set_title(f\"SVM with C={model.C}\")\n",
        "\n",
        "# Plot decision boundaries for different C values\n",
        "fig, axes = plt.subplots(1, len(C_values), figsize=(15, 4))\n",
        "\n",
        "for ax, C in zip(axes, C_values):\n",
        "    svm = SVC(kernel='linear', C=C)\n",
        "    svm.fit(X_train, y_train)\n",
        "    plot_decision_boundary(svm, X, y, ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VOQR5ZnFR8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1a7avodQR5U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28.  Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "# binary features"
      ],
      "metadata": {
        "id": "kFiqF6CPSAXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NNxBbVOWR-Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate a synthetic dataset with binary features\n",
        "np.random.seed(42)\n",
        "X = np.random.randint(2, size=(500, 10))  # 500 samples, 10 binary features\n",
        "y = np.random.randint(2, size=500)  # Binary target labels (0 or 1)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bernoulli Naïve Bayes classifier\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot confusion matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QzRELWDv69nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28. = Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "#unscaled data"
      ],
      "metadata": {
        "id": "ByH0TMWMSPq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM model on unscaled data\n",
        "svm_unscaled = SVC(kernel='rbf', C=1)\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM model on scaled data\n",
        "svm_scaled = SVC(kernel='rbf', C=1)\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print accuracy results\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title, ax):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "    ax.set_title(title)\n",
        "\n",
        "# Plot decision boundaries for both models\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_decision_boundary(svm_unscaled, X_train, y_train, \"SVM without Scaling\", axes[0])\n",
        "plot_decision_boundary(svm_scaled, X_train_scaled, y_train, \"SVM with Scaling\", axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "affXAWKzSiLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#28. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
        "#after Laplace Smoothing"
      ],
      "metadata": {
        "id": "a6TLa4--SjVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes model without Laplace smoothing (var_smoothing=0)\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=0)\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "# Train Gaussian Naïve Bayes model with Laplace smoothing (var_smoothing=1e-9)\n",
        "gnb_smoothing = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_smoothing.fit(X_train, y_train)\n",
        "y_pred_smoothing = gnb_smoothing.predict(X_test)\n",
        "accuracy_smoothing = accuracy_score(y_test, y_pred_smoothing)\n",
        "\n",
        "# Print accuracy results\n",
        "print(f\"Accuracy without Laplace Smoothing: {accuracy_no_smoothing:.4f}\")\n",
        "print(f\"Accuracy with Laplace Smoothing: {accuracy_smoothing:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (No Smoothing):\")\n",
        "print(classification_report(y_test, y_pred_no_smoothing))\n",
        "\n",
        "print(\"\\nClassification Report (With Smoothing):\")\n",
        "print(classification_report(y_test, y_pred_smoothing))\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title, ax):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "    ax.set_title(title)\n",
        "\n",
        "# Plot decision boundaries\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_decision_boundary(gnb_no_smoothing, X_train, y_train, \"GNB Without Smoothing\", axes[0])\n",
        "plot_decision_boundary(gnb_smoothing, X_train, y_train, \"GNB With Smoothing\", axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sAh5KcihUEra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  31 .Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "#gamma, kernel)"
      ],
      "metadata": {
        "id": "ZcNVmkwHUIZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],      # Regularization parameter\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Different kernel types\n",
        "}\n",
        "\n",
        "# Create an SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print best parameters and best score\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Training Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Train the best SVM model on the full training set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot confusion matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H66b8H6xUQZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy"
      ],
      "metadata": {
        "id": "cRzeWbreUgPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM classifier without class weighting\n",
        "svm_no_weighting = SVC(kernel='rbf', C=1, random_state=42)\n",
        "svm_no_weighting.fit(X_train_scaled, y_train)\n",
        "y_pred_no_weighting = svm_no_weighting.predict(X_test_scaled)\n",
        "accuracy_no_weighting = accuracy_score(y_test, y_pred_no_weighting)\n",
        "\n",
        "# Train an SVM classifier with class weighting\n",
        "svm_weighted = SVC(kernel='rbf', C=1, class_weight='balanced', random_state=42)\n",
        "svm_weighted.fit(X_train_scaled, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test_scaled)\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "# Print accuracy results\n",
        "print(f\"Accuracy without Class Weighting: {accuracy_no_weighting:.4f}\")\n",
        "print(f\"Accuracy with Class Weighting: {accuracy_weighted:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (No Weighting):\")\n",
        "print(classification_report(y_test, y_pred_no_weighting))\n",
        "\n",
        "print(\"\\nClassification Report (With Class Weighting):\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title, ax):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "    ax.set_title(title)\n",
        "\n",
        "# Plot decision boundaries\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_decision_boundary(svm_no_weighting, X_train_scaled, y_train, \"SVM Without Class Weighting\", axes[0])\n",
        "plot_decision_boundary(svm_weighted, X_train_scaled, y_train, \"SVM With Class Weighting\", axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QwsifWiUUrsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#33 . = Write a Python program to implement a Naïve Bayes classifier for spam detection using email data"
      ],
      "metadata": {
        "id": "7Un7XSBYUvft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Load dataset (you can replace with any email dataset)\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms-spam-collection.csv\"\n",
        "df = pd.read_csv(url, encoding=\"latin-1\")\n",
        "df.columns = [\"label\", \"message\"]\n",
        "\n",
        "# Convert labels to binary values (ham=0, spam=1)\n",
        "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# Display data distribution\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# Text Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply text preprocessing\n",
        "df[\"message\"] = df[\"message\"].apply(preprocess_text)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"message\"], df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to numerical representation using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=3000)  # Limit features to 3000\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naïve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cvp0I0FMU4gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 34. = Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "#compare their accuracy"
      ],
      "metadata": {
        "id": "1SyoxTzpU8fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM classifier\n",
        "svm_model = SVC(kernel='rbf', C=1, random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Train a Naïve Bayes classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)  # No scaling needed for Naïve Bayes\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the models\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"Accuracy (SVM): {accuracy_svm:.4f}\")\n",
        "print(f\"Accuracy (Naïve Bayes): {accuracy_nb:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (SVM):\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "print(\"\\nClassification Report (Naïve Bayes):\")\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title, ax, scaler=None):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    if scaler:\n",
        "        X_grid = scaler.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "    else:\n",
        "        X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    Z = model.predict(X_grid)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "    ax.set_title(title)\n",
        "\n",
        "# Plot decision boundaries\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_decision_boundary(svm_model, X_train_scaled, y_train, \"SVM Decision Boundary\", axes[0], scaler=scaler)\n",
        "plot_decision_boundary(nb_model, X_train, y_train, \"Naïve Bayes Decision Boundary\", axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UOypk_h8VHWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "#results"
      ],
      "metadata": {
        "id": "GB3gJpk3KOKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Load dataset (SMS Spam Collection dataset)\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms-spam-collection.csv\"\n",
        "df = pd.read_csv(url, encoding=\"latin-1\")\n",
        "df.columns = [\"label\", \"message\"]\n",
        "\n",
        "# Convert labels to binary values (ham=0, spam=1)\n",
        "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# Text Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = text.strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply text preprocessing\n",
        "df[\"message\"] = df[\"message\"].apply(preprocess_text)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"message\"], df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to numerical representation using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)  # Limit features to 5000\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Feature Selection using Chi-Square Test\n",
        "num_features = 1000  # Select the top 1000 best features\n",
        "selector = SelectKBest(chi2, k=num_features)\n",
        "X_train_selected = selector.fit_transform(X_train_tfidf, y_train)\n",
        "X_test_selected = selector.transform(X_test_tfidf)\n",
        "\n",
        "# Train Naïve Bayes classifier (Baseline - All Features)\n",
        "nb_baseline = MultinomialNB()\n",
        "nb_baseline.fit(X_train_tfidf, y_train)\n",
        "y_pred_baseline = nb_baseline.predict(X_test_tfidf)\n",
        "\n",
        "# Train Naïve Bayes classifier (With Feature Selection)\n",
        "nb_selected = MultinomialNB()\n",
        "nb_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = nb_selected.predict(X_test_selected)\n",
        "\n",
        "# Evaluate Performance\n",
        "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "print(f\"Accuracy (All Features): {accuracy_baseline:.4f}\")\n",
        "print(f\"Accuracy (Selected Features): {accuracy_selected:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (All Features):\")\n",
        "print(classification_report(y_test, y_pred_baseline))\n",
        "\n",
        "print(\"\\nClassification Report (Selected Features):\")\n",
        "print(classification_report(y_test, y_pred_selected))\n",
        "\n",
        "# Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_baseline, cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix (All Features)\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_selected, cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix (Selected Features)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ttqJIOBJLT_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "# strategies on the Wine dataset and compare their accuracy\n"
      ],
      "metadata": {
        "id": "eQlsS24rLVIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GrKLXjhPUrMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM Classifier using One-vs-Rest (OvR)\n",
        "svm_ovr = SVC(kernel=\"linear\", decision_function_shape=\"ovr\", random_state=42)\n",
        "svm_ovr.fit(X_train_scaled, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test_scaled)\n",
        "\n",
        "# Train SVM Classifier using One-vs-One (OvO)\n",
        "svm_ovo = SVC(kernel=\"linear\", decision_function_shape=\"ovo\", random_state=42)\n",
        "svm_ovo.fit(X_train_scaled, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Performance\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "print(f\"Accuracy (OvR): {accuracy_ovr:.4f}\")\n",
        "print(f\"Accuracy (OvO): {accuracy_ovo:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (OvR):\")\n",
        "print(classification_report(y_test, y_pred_ovr))\n",
        "\n",
        "print(\"\\nClassification Report (OvO):\")\n",
        "print(classification_report(y_test, y_pred_ovo))\n",
        "\n",
        "# Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_ovr, cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix (OvR)\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_ovo, cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix (OvO)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IDy2ZxWkLfrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "# Cancer dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "h9DBuxY1LvjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM Classifier with Linear Kernel\n",
        "svm_linear = SVC(kernel=\"linear\", C=1, random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "\n",
        "# Train SVM Classifier with Polynomial Kernel\n",
        "svm_poly = SVC(kernel=\"poly\", degree=3, C=1, random_state=42)\n",
        "svm_poly.fit(X_train_scaled, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test_scaled)\n",
        "\n",
        "# Train SVM Classifier with RBF Kernel\n",
        "svm_rbf = SVC(kernel=\"rbf\", C=1, gamma=\"scale\", random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Performance\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy (Linear Kernel): {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy (Polynomial Kernel): {accuracy_poly:.4f}\")\n",
        "print(f\"Accuracy (RBF Kernel): {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (Linear Kernel):\")\n",
        "print(classification_report(y_test, y_pred_linear))\n",
        "\n",
        "print(\"\\nClassification Report (Polynomial Kernel):\")\n",
        "print(classification_report(y_test, y_pred_poly))\n",
        "\n",
        "print(\"\\nClassification Report (RBF Kernel):\")\n",
        "print(classification_report(y_test, y_pred_rbf))\n",
        "\n",
        "# Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_linear, cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix (Linear Kernel)\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_poly, cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix (Polynomial Kernel)\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rbf, cmap=\"Blues\", ax=axes[2])\n",
        "axes[2].set_title(\"Confusion Matrix (RBF Kernel)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Reduce Data to 2D using PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train models again on PCA-reduced features for visualization\n",
        "svm_linear_pca = SVC(kernel=\"linear\", C=1, random_state=42)\n",
        "svm_linear_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "svm_poly_pca = SVC(kernel=\"poly\", degree=3, C=1, random_state=42)\n",
        "svm_poly_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "svm_rbf_pca = SVC(kernel=\"rbf\", C=1, gamma=\"scale\", random_state=42)\n",
        "svm_rbf_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title, ax):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    Z = model.predict(X_grid)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)\n",
        "    ax.set_title(title)\n",
        "\n",
        "# Plot decision boundaries\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "plot_decision_boundary(svm_linear_pca, X_train_pca, y_train, \"SVM Linear Kernel\", axes[0])\n",
        "plot_decision_boundary(svm_poly_pca, X_train_pca, y_train, \"SVM Polynomial Kernel\", axes[1])\n",
        "plot_decision_boundary(svm_rbf_pca, X_train_pca, y_train, \"SVM RBF Kernel\", axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AC5WTX7LMDLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n"
      ],
      "metadata": {
        "id": "fDy2T3gNMT53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Standardize the features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the SVM Classifier\n",
        "svm_model = SVC(kernel=\"linear\", C=1, random_state=42)\n",
        "\n",
        "# Set up Stratified K-Fold Cross-Validation (K=5)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and compute accuracy for each fold\n",
        "cv_scores = cross_val_score(svm_model, X_scaled, y, cv=kfold, scoring=\"accuracy\")\n",
        "\n",
        "# Print individual fold accuracies\n",
        "print(f\"Cross-validation accuracies: {cv_scores}\")\n",
        "\n",
        "# Compute and print the average accuracy\n",
        "average_accuracy = np.mean(cv_scores)\n",
        "print(f\"\\nAverage Accuracy: {average_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "xran67biMdru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "# performance"
      ],
      "metadata": {
        "id": "Ow6os4X_MnVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Naïve Bayes Classifier with default priors (equal probabilities)\n",
        "nb_default = GaussianNB()\n",
        "nb_default.fit(X_train, y_train)\n",
        "y_pred_default = nb_default.predict(X_test)\n",
        "\n",
        "# Define custom prior probabilities (modify as needed)\n",
        "custom_priors = [0.3, 0.7]  # Example: Prior belief that class 1 is more common\n",
        "nb_custom = GaussianNB(priors=custom_priors)\n",
        "nb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = nb_custom.predict(X_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "print(f\"Accuracy (Default Priors): {accuracy_default:.4f}\")\n",
        "print(f\"Accuracy (Custom Priors): {accuracy_custom:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (Default Priors):\")\n",
        "print(classification_report(y_test, y_pred_default))\n",
        "\n",
        "print(\"\\nClassification Report (Custom Priors):\")\n",
        "print(classification_report(y_test, y_pred_custom))\n",
        "\n",
        "# Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_default, cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix (Default Priors)\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_custom, cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix (Custom Priors)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DBzH6z4QMtSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "# compare accuracy"
      ],
      "metadata": {
        "id": "JA2yGY49M24y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM Classifier with All Features\n",
        "svm_full = SVC(kernel=\"linear\", C=1, random_state=42)\n",
        "svm_full.fit(X_train_scaled, y_train)\n",
        "y_pred_full = svm_full.predict(X_test_scaled)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE) to select top features\n",
        "num_features_to_select = 10  # Change this value to select a different number of features\n",
        "rfe = RFE(estimator=SVC(kernel=\"linear\", C=1), n_features_to_select=num_features_to_select)\n",
        "rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Select only the most important features\n",
        "X_train_rfe = rfe.transform(X_train_scaled)\n",
        "X_test_rfe = rfe.transform(X_test_scaled)\n",
        "\n",
        "# Train SVM Classifier with Selected Features (RFE)\n",
        "svm_rfe = SVC(kernel=\"linear\", C=1, random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "\n",
        "# Evaluate Performance\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "print(f\"Accuracy (All Features): {accuracy_full:.4f}\")\n",
        "print(f\"Accuracy (Selected Features via RFE): {accuracy_rfe:.4f}\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nClassification Report (All Features):\")\n",
        "print(classification_report(y_test, y_pred_full))\n",
        "\n",
        "print(\"\\nClassification Report (Selected Features via RFE):\")\n",
        "print(classification_report(y_test, y_pred_rfe))\n",
        "\n",
        "# Plot Feature Importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(len(rfe.ranking_)), rfe.ranking_)\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"RFE Ranking (1 = Important)\")\n",
        "plt.title(\"Feature Importance Ranking via RFE\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JNJycjceNHyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 41 Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "#F1-Score instead of accuracy"
      ],
      "metadata": {
        "id": "eMGhtzyiNUfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM Classifier\n",
        "svm_model = SVC(kernel=\"linear\", C=1, random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Compute Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Display Metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Display Confusion Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hZ6D6_rmNblC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 42. = Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using LogLoss\n",
        "#(Cross-Entropy Loss)"
      ],
      "metadata": {
        "id": "nqfDDhBjNgWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Naïve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict Probabilities (for Log Loss computation)\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Compute Log Loss (Cross-Entropy Loss)\n",
        "log_loss_value = log_loss(y_test, y_prob)\n",
        "\n",
        "# Display Log Loss\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {log_loss_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "Trz-yiqYNwR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 43.  Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn"
      ],
      "metadata": {
        "id": "QQ3UwCOtN0Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVM Classifier\n",
        "svm_model = SVC(kernel=\"linear\", C=1, random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize Confusion Matrix using Seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Benign\", \"Malignant\"], yticklabels=[\"Benign\", \"Malignant\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix for SVM Classifier\")\n",
        "plt.show()\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "pYVkEJFTORIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "# Error (MAE) instead of MSE="
      ],
      "metadata": {
        "id": "m7UbZlrmOdLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target  # Features and target values\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for SVR)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train an SVR model with RBF Kernel\n",
        "svr_model = SVR(kernel=\"rbf\", C=100, gamma=0.1, epsilon=0.1)\n",
        "svr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the target values\n",
        "y_pred = svr_model.predict(X_test_scaled)\n",
        "\n",
        "# Compute Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Display MAE\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "# Plot Actual vs Predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color=\"blue\", alpha=0.6, label=\"Predictions\")\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"dashed\", linewidth=2, label=\"Perfect Fit\")\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"SVR Predictions vs Actual Values\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PYc9iZVJOc6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "# score"
      ],
      "metadata": {
        "id": "qlSB29Kbhuse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Naïve Bayes classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class (label = 1)\n",
        "y_probs = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# Display ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")  # Diagonal baseline\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve for Naïve Bayes Classifier\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BWWVK0Rehx_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#46. = Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve"
      ],
      "metadata": {
        "id": "JUdUgbPHiA5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # Features and labels\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with probability estimation enabled\n",
        "svm_model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class (label = 1)\n",
        "y_probs = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Compute AUC-PR (Area Under the Precision-Recall Curve)\n",
        "auc_pr = auc(recall, precision)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color=\"blue\", lw=2, label=f\"PR Curve (AUC = {auc_pr:.4f})\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve for SVM Classifier\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_pj7RMMmiKyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6NBSXAMEUF5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GK2jFhFwSh0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZ3I5xfcSaWx"
      }
    }
  ]
}